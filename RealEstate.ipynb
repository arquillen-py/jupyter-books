{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression Challenge\r\n",
    "\r\n",
    "Predicting the selling price of a residential property depends on a number of factors, including the property age, availability of local amenities, and location.\r\n",
    "\r\n",
    "In this challenge, you will use a dataset of real estate sales transactions to predict the price-per-unit of a property based on its features. The price-per-unit in this data is based on a unit measurement of 3.3 square meters.\r\n",
    "\r\n",
    "> **Citation**: The data used in this exercise originates from the following study:\r\n",
    ">\r\n",
    "> *Yeh, I. C., & Hsu, T. K. (2018). Building real estate valuation models with comparative approach through case-based reasoning. Applied Soft Computing, 65, 260-271.*\r\n",
    ">\r\n",
    "> It was obtained from the UCI dataset repository (Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data consists of the following variables:\r\n",
    "\r\n",
    "- **transaction_date** - the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)\r\n",
    "- **house_age** - the house age (in years)\r\n",
    "- **transit_distance** - the distance to the nearest light rail station (in meters)\r\n",
    "- **local_convenience_stores** - the number of convenience stores within walking distance\r\n",
    "- **latitude** - the geographic coordinate, latitude\r\n",
    "- **longitude** - the geographic coordinate, longitude\r\n",
    "- **price_per_unit** house price of unit area (3.3 square meters)\r\n",
    "\r\n",
    "## The Challenge\r\n",
    "\r\n",
    "Your challenge is to explore and prepare the data, identify predictive features that will help predict the **price_per_unit** label, and train a regression model that achieves the lowest Root Mean Square Error (RMSE) you can achieve (which must be less than **7**) when evaluated against a test subset of data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "try:\r\n",
    "    import folium\r\n",
    "except ModuleNotFoundError:\r\n",
    "    !pip install folium\r\n",
    "    import folium\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "try:\r\n",
    "    import plotly.express as px\r\n",
    "except ModuleNotFoundError:\r\n",
    "    !pip install plotly\r\n",
    "    import plotly.express as px\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib as mpl\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error, r2_score\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\r\n",
    "\r\n",
    "try:\r\n",
    "    import statsmodels.api as sm\r\n",
    "    import statsmodels.formula.api as smf\r\n",
    "except ModuleNotFoundError:\r\n",
    "    !pip install statsmodels\r\n",
    "    import statsmodels.api as sm\r\n",
    "    import statsmodels.formula.api as smf\r\n",
    "from statsmodels.tools.eval_measures import rmse\r\n",
    "\r\n",
    "import joblib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Review the data\r\n",
    "\r\n",
    "Run the following cell to load the data and view the first few rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load the training dataset\r\n",
    "data = pd.read_csv('Datasets/real_estate.csv')\r\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check shape of datasets\r\n",
    "print('The dataset has %d rows and %d columns.' % (data.shape[0], data.shape[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check for missing values\r\n",
    "data.isnull().sum() # No NA's"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This next section will turn the 'transaction date' into a YYYY-MM-DD format, and add it as a new column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Turn date into a more readable format\r\n",
    "\r\n",
    "dictMonth = {}\r\n",
    "for i in range(1, 13):\r\n",
    "    dictMonth.update({round(i/12, 3):i})\r\n",
    "#print(dictMonth)\r\n",
    "\r\n",
    "fixedDate = []\r\n",
    "for date in data['transaction_date']:\r\n",
    "    if (float(\"0.\" + str(date).split(\".\")[1])) == 0.0:\r\n",
    "        fixedDate.append(pd.to_datetime(\"12/\" + str(int(str(date).split(\".\")[0]) - 1), format = '%m/%Y', errors = 'coerce'))\r\n",
    "    else:\r\n",
    "        fixedDate.append(pd.to_datetime(str(dictMonth[round(float(\"0.\" + str(date).split(\".\")[1]), 3)]) + \"/\"  + str(date).split(\".\")[0], format = '%m/%Y', errors = 'coerce'))\r\n",
    "    \r\n",
    "data[\"transaction_date_formatted\"] = fixedDate\r\n",
    "\r\n",
    "data.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll get a sense of the distributions of the variables. Note the outliers in the box-and-whiskers plot of 'price per unit'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (12,15))\r\n",
    "\r\n",
    "ax[0].hist(data[\"price_per_unit\"])\r\n",
    "ax[0].set_title(\"Histogram of Price per unit\")\r\n",
    "\r\n",
    "ax[1].boxplot(data[\"price_per_unit\"], vert = False)\r\n",
    "ax[1].set_title(\"Box-and-whiskers of Price per unit\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (12,15))\r\n",
    "\r\n",
    "ax[0].hist(data[\"transaction_date_formatted\"])\r\n",
    "ax[0].set_title(\"Histogram of Transactions by Date\")\r\n",
    "\r\n",
    "ax[1].boxplot(data[\"transaction_date_formatted\"].dt.month, vert = False)\r\n",
    "ax[1].set_title(\"Box-and-whiskers of Transactions by Month\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\r\n",
    "\r\n",
    "ax.set_title('Label by ' + \"Transaction Data (formatted)\")\r\n",
    "ax.set_ylabel(\"Price per unit\")\r\n",
    "\r\n",
    "data.boxplot(column = 'price_per_unit', by = 'transaction_date_formatted', ax = ax, rot = 20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.box(data, x = data[\"house_age\"])\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.box(data, x = data[\"local_convenience_stores\"])\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By plotting the latitude and longitude we can see where the transactions took place. They all occurred in Taiwan."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tw_map = folium.Map(location = [24.98, 121.55], zoom_start = 12)\r\n",
    "\r\n",
    "transactions = folium.map.FeatureGroup()\r\n",
    "\r\n",
    "for lat, lng, in zip(data[\"latitude\"], data[\"longitude\"]):\r\n",
    "    transactions.add_child(\r\n",
    "        folium.features.CircleMarker(\r\n",
    "            [lat, lng],\r\n",
    "            radius=5,\r\n",
    "            color='yellow',\r\n",
    "            fill=True,\r\n",
    "            fill_color='blue',\r\n",
    "            fill_opacity=0.6\r\n",
    "        )\r\n",
    "    )\r\n",
    "tw_map.add_child(transactions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also get a feel for which variables will be useful in the regression by checking the correlations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numeric_cols = [\"house_age\", \"transit_distance\", \"latitude\", \"longitude\"]\r\n",
    "\r\n",
    "for col in numeric_cols:\r\n",
    "    fig = plt.figure(figsize=(9, 6))\r\n",
    "    ax = fig.gca()\r\n",
    "    feature = data[col]\r\n",
    "    label = data['price_per_unit']\r\n",
    "    correlation = feature.corr(label)\r\n",
    "    plt.scatter(x=feature, y=label)\r\n",
    "    plt.xlabel(col)\r\n",
    "    plt.ylabel('Price per unit')\r\n",
    "    ax.set_title('Price per unit vs ' + col + '- correlation: ' + str(correlation))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Model\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "v = OneHotEncoder(sparse=False)\r\n",
    "\r\n",
    "dateCols =  pd.DataFrame(v.fit_transform(data[[\"transaction_date_formatted\"]]), columns = v.get_feature_names())\r\n",
    " \r\n",
    "data = pd.concat([data, dateCols], axis=1)\r\n",
    "\r\n",
    "for i in range(8, 20):\r\n",
    "    if i < 13:\r\n",
    "        data = data.rename(columns = {data.columns[i]: str(i) + \"/2012\"})\r\n",
    "    else:\r\n",
    "        data = data.rename(columns = {data.columns[i]: str(i-12) + \"/2013\"})\r\n",
    "\r\n",
    "data.head()\r\n",
    "'''\r\n",
    "# Dropping the outliers\r\n",
    "data = data[data['price_per_unit']<70]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mapping the date to its corresponding season:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\r\n",
    "\r\n",
    "month_to_season = dict(zip(range(1,13), seasons))\r\n",
    "\r\n",
    "data[\"season\"] = data.transaction_date_formatted.dt.month.map(month_to_season) \r\n",
    "\r\n",
    "data = data.drop('transaction_date', axis = 1)\r\n",
    "data = data.drop('transaction_date_formatted', axis = 1)\r\n",
    "\r\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our label and features:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = data[[\"house_age\", \"transit_distance\", \"latitude\", \"longitude\", \"season\"]]\r\n",
    "y = data['price_per_unit']\r\n",
    "\r\n",
    "print('Features:',X[:5], '\\nLabels:', y[:5], sep='\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting the data into training (70%) and testing (30%)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = [c for c in data.columns if c != \"price_per_unit\"]\r\n",
    "\r\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The hyperparameters that we will be testing for our RandomForestRegressor; the GridSearchCV function will find us the combination resulting in the lowest R-squared."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    'n_estimators': [1, 2, 5, 10, 25, 50, 100],\r\n",
    "    'min_samples_split': [2, 10, 20, 50, 100, 500],\r\n",
    "    'max_depth': [2, 4, 6, 8, 10, 15, 20, 50, 100]\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_forest = RandomForestRegressor()\r\n",
    "\r\n",
    "rdm = GridSearchCV(random_forest, parameters, scoring = 'r2')\r\n",
    "\r\n",
    "rdm.fit(train[features], train.price_per_unit)\r\n",
    "\r\n",
    "best_params = rdm.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline will scale our numeric features and one-hot encode our categorical (season). We then give the regressor the best parameters and fit the model on our training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define preprocessing for numeric columns (scale them)\r\n",
    "numeric_features = [0,1,2,3]\r\n",
    "numeric_transformer = Pipeline(steps=[\r\n",
    "    ('scaler', StandardScaler())])\r\n",
    "\r\n",
    "categorical_features = [4]\r\n",
    "categorical_transformer = Pipeline(steps=[\r\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n",
    "\r\n",
    "# Combine preprocessing steps\r\n",
    "preprocessor = ColumnTransformer(\r\n",
    "    transformers=[\r\n",
    "        ('num', numeric_transformer, numeric_features),\r\n",
    "        ('cat', categorical_transformer, categorical_features)\r\n",
    "    ])\r\n",
    "\r\n",
    "# Create preprocessing and training pipeline\r\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
    "                           ('regressor', RandomForestRegressor(max_depth=best_params['max_depth'],\r\n",
    "                                                               min_samples_split=best_params['min_samples_split'],\r\n",
    "                                                               n_estimators=best_params['n_estimators']))])\r\n",
    "\r\n",
    "\r\n",
    "# fit the pipeline to train a linear regression model on the training set\r\n",
    "model = pipeline.fit(train[features], (train.price_per_unit))\r\n",
    "print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that we fell well below the 7 threshold for root mean squared error, and achieved an R-squared of .7."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get predictions\r\n",
    "predictions = model.predict(test[features])\r\n",
    "\r\n",
    "# Display metrics\r\n",
    "mse = mean_squared_error(test.price_per_unit, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "r2 = r2_score(test.price_per_unit, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "# Plot predicted vs actual\r\n",
    "plt.scatter(test.price_per_unit, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions vs Actuals')\r\n",
    "z = np.polyfit(test.price_per_unit, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(test.price_per_unit, p(test.price_per_unit), color='magenta')\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the Trained Model\n",
    "\n",
    "Save your trained model, and then use it to predict the price-per-unit for the following real estate transactions:\n",
    "\n",
    "| transaction_date | house_age | transit_distance | local_convenience_stores | latitude | longitude |\n",
    "| ---------------- | --------- | ---------------- | ------------------------ | -------- | --------- |\n",
    "|2013.167|16.2|289.3248|5|24.98203|121.54348|\n",
    "|2013.000|13.6|4082.015|0|24.94155|121.50381|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the model as a pickle file\r\n",
    "filename = './real_estate_model.pkl'\r\n",
    "joblib.dump(model, filename)\r\n",
    "\r\n",
    "# Load the model from the file\r\n",
    "loaded_model = joblib.load(filename)\r\n",
    "\r\n",
    "# An array of features for each transaction (don't include the transaction date)\r\n",
    "X_new = np.array([[16.2,289.3248,5,24.98203,121.54348,4],\r\n",
    "                  [13.6,4082.015,0,24.94155,121.5038,2]])\r\n",
    "\r\n",
    "# Use the model to predict unit price\r\n",
    "results = loaded_model.predict(X_new)\r\n",
    "print('Predictions:')\r\n",
    "for prediction in results:\r\n",
    "    print(round(prediction,2))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "b2c5e44d31b875738ed638c71f6d3072afbbc08fcbfca63303afa3e2eb11ddc0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}