{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Metacritic Comments and Scores\r\n",
                "This dataset is a web scrape of Metacritic user comments and reviews by Dahlia at kaggle.com. Metacritic is a review aggregator for movies, TV shows, music albums, and video games.\r\n",
                "\r\n",
                "Source: https://www.kaggle.com/dahlia25/metacritic-video-game-comments, accessed on July 26th, 2020.\r\n",
                "\r\n",
                "The web scraper can be found at https://github.com/dahlia25/game_recommender, and uses Selenium and BeautifulSoup.\r\n",
                "\r\n",
                "The data range between 1998 and 2018."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## The Dataset\r\n",
                "Number of Rows | Number of Columns | One Row Represents\r\n",
                "------------- | ------------- | -------------\r\n",
                "283,983 | 5 | One user review"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Import necessary packages\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "\r\n",
                "try:\r\n",
                "    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\r\n",
                "    print(\"Module 'wordcloud' is installed\")\r\n",
                "except ModuleNotFoundError:\r\n",
                "    %pip install wordcloud\r\n",
                "    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\r\n",
                "\r\n",
                "try:\r\n",
                "    import nltk\r\n",
                "    print(\"Module 'nltk' is installed\")\r\n",
                "except ModuleNotFoundError:\r\n",
                "    %pip install nltk\r\n",
                "    import nltk\r\n",
                "from nltk.corpus import twitter_samples\r\n",
                "from nltk.tag import pos_tag\r\n",
                "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
                "from nltk.corpus import stopwords\r\n",
                "from nltk import FreqDist\r\n",
                "from nltk import classify\r\n",
                "from nltk import NaiveBayesClassifier\r\n",
                "from nltk.tokenize import TweetTokenizer\r\n",
                "from nltk.corpus import wordnet\r\n",
                "from nltk.sentiment import SentimentIntensityAnalyzer\r\n",
                "\r\n",
                "import re, string\r\n",
                "\r\n",
                "import random"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def remove_noise(tweet_tokens, stop_words = ()):\r\n",
                "\r\n",
                "    cleaned_tokens = []\r\n",
                "\r\n",
                "    for token, tag in pos_tag(tweet_tokens):\r\n",
                "        token = re.sub(\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"\", token) #Remove hyperlinks of http[s] variety\r\n",
                "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token) #Remove Twitter @'s\r\n",
                "        if tag.startswith(\"NN\"):\r\n",
                "            pos = wordnet.NOUN\r\n",
                "        elif tag.startswith(\"VB\"):\r\n",
                "            pos = wordnet.VERB\r\n",
                "        elif tag.startswith(\"JJ\"):\r\n",
                "            pos = wordnet.ADJ\r\n",
                "        else:\r\n",
                "            pos = wordnet.ADV\r\n",
                "\r\n",
                "        lemmatizer = WordNetLemmatizer()\r\n",
                "        token = lemmatizer.lemmatize(token, pos)\r\n",
                "\r\n",
                "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\r\n",
                "            cleaned_tokens.append(token.lower())\r\n",
                "    return cleaned_tokens\r\n",
                "\r\n",
                "def get_all_words(cleaned_tokens_list):\r\n",
                "    for tokens in cleaned_tokens_list:\r\n",
                "        for token in tokens:\r\n",
                "            yield token\r\n",
                "\r\n",
                "def get_tweets_for_model(cleaned_tokens_list):\r\n",
                "    for tokens in cleaned_tokens_list:\r\n",
                "        yield dict([token, True] for token in tokens)\r\n",
                "\r\n",
                "def is_positive(token):\r\n",
                "    return sia.polarity_scores(token)[\"compound\"] > 0\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "nltk.download(\"twitter_samples\")\r\n",
                "nltk.download(\"punkt\")\r\n",
                "nltk.download('wordnet')\r\n",
                "nltk.download('averaged_perceptron_tagger')\r\n",
                "nltk.download('stopwords')\r\n",
                "nltk.download('vader_lexicon')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dfN64 = pd.read_csv(\"Datasets/n64.csv\", index_col = 0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "stop_words = stopwords.words('english')\r\n",
                "\r\n",
                "tokenizer = TweetTokenizer()\r\n",
                "\r\n",
                "#n64Text = dfN64[\"Comment\"].str.cat(sep = \" \")\r\n",
                "n64Comments = []\r\n",
                "for comment in dfN64[\"Comment\"]:\r\n",
                "    n64Comments.append(tokenizer.tokenize((comment)))\r\n",
                "\r\n",
                "#positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\r\n",
                "#negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\r\n",
                "#text = twitter_samples.strings(\"tweets.20150430-223406.json\")\r\n",
                "\r\n",
                "#positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\r\n",
                "#negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\r\n",
                "\r\n",
                "print(n64Comments[0])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(pos_tag(n64Comments[0]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "cleanedN64Comments = []\r\n",
                "for token in n64Comments:\r\n",
                "    cleanedN64Comments.append(remove_noise(token, stop_words))\r\n",
                "print(cleanedN64Comments[0])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "allN64WordsGen = get_all_words(cleanedN64Comments)\r\n",
                "allN64Words = []\r\n",
                "for token in allN64WordsGen:\r\n",
                "    allN64Words.append(token)\r\n",
                "#print(allN64Words)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def get_tweets_for_model(cleaned_tokens_list):\r\n",
                "    for tokens in cleaned_tokens_list:\r\n",
                "        yield dict([token, True] for token in tokens)\r\n",
                "\r\n",
                "n64ForModel = get_tweets_for_model(cleanedN64Comments)\r\n",
                "\r\n",
                "sia = SentimentIntensityAnalyzer()\r\n",
                "\r\n",
                "allN64WordsGen = get_all_words(cleanedN64Comments)\r\n",
                "positiveN64 = [(token, \"Positive\") for token in allN64WordsGen if sia.polarity_scores(token)[\"compound\"] > 0]\r\n",
                "\r\n",
                "allN64WordsGen = get_all_words(cleanedN64Comments)\r\n",
                "negativeN64 = [(token, \"Negative\") for token in allN64WordsGen if sia.polarity_scores(token)[\"compound\"] <= 0]\r\n",
                "\r\n",
                "allN64WordsGen = get_all_words(cleanedN64Comments)\r\n",
                "neutralN64 = [(token, \"Neutral\") for token in allN64WordsGen if sia.polarity_scores(token)[\"compound\"] == 0]\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "#print(neutralN64[0])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dataset = positiveN64 + negativeN64 + neutralN64\r\n",
                "\r\n",
                "random.shuffle(dataset)\r\n",
                "\r\n",
                "dataset.shape[0]\r\n",
                "\r\n",
                "#train_data = dataset[:7000]\r\n",
                "#test_data = dataset[7000:]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#positiveFreqDist = FreqDist(allN64Words)\r\n",
                "#print(positiveFreqDist.most_common(10))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "allN64WordsString = \" \".join(allN64Words)\r\n",
                "sia = SentimentIntensityAnalyzer()\r\n",
                "sia.polarity_scores(allN64WordsString)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "positive_tokens_for_model = get_tweets_for_model(cleanedPositiveTweetsList)\r\n",
                "negative_tokens_for_model = get_tweets_for_model(cleanedNegativeTweetsList)\r\n",
                "\r\n",
                "positive_dataset = [(token, \"Positive\")\r\n",
                "                     for token in positive_tokens_for_model]\r\n",
                "print(positive_dataset[0])\r\n",
                "\r\n",
                "\r\n",
                "negative_dataset = [(token, \"Negative\")\r\n",
                "                     for token in negative_tokens_for_model]\r\n",
                "print(negative_dataset[0])\r\n",
                "\r\n",
                "dataset = positive_dataset + negative_dataset\r\n",
                "\r\n",
                "random.shuffle(dataset)\r\n",
                "\r\n",
                "train_data = dataset[:7000]\r\n",
                "test_data = dataset[7000:]\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "classifier = NaiveBayesClassifier.train(train_data)\r\n",
                "\r\n",
                "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\r\n",
                "\r\n",
                "print(classifier.show_most_informative_features(10))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#df = pd.read_csv(\"D:/Git/large datasets/metacritic_game_user_comments.csv\", index_col = 0)\r\n",
                "#dfN64 = df[df[\"Platform\"] == \"Nintendo64\"]\r\n",
                "#dfN64.to_csv(\"D:/Git/jupyter-books/Datasets/n64.csv\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dfN64 = pd.read_csv(\"Datasets/n64.csv\", index_col = 0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "dfN64.head()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "n64Text = dfN64[\"Comment\"].str.cat(sep = \" \")\r\n",
                "\r\n",
                "stopwords = set(STOPWORDS)\r\n",
                "stopwords.update([\"game\", \"play\", \"played\", \"to\", \"feel\", \"even\", \"want\", \"make\", \"made\", \"much\", \"still\", \"playing\", \"one\", \"games\"])\r\n",
                "\r\n",
                "cloud = WordCloud(width = 1200, height = 800, stopwords = stopwords, background_color = \"white\").generate(n64Text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plt.figure(figsize = (19.2, 10.8))\r\n",
                "plt.imshow(cloud, interpolation = \"bilinear\")\r\n",
                "plt.axis(\"off\")\r\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "b2c5e44d31b875738ed638c71f6d3072afbbc08fcbfca63303afa3e2eb11ddc0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}